---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a junior undergraduate at [Department of Management Science and Engineering](https://www.sem.tsinghua.edu.cn/mseen/) of [Tsinghua University](https://www.tsinghua.edu.cn/en/), majoring in Information Management and Information Systems with a minor in Statistical Science. I'm now doing research in the intersection of machine learning and operation research, fortunately having [Prof.Qin Hanzhang](https://hanzhangqin.com/) as my research advisor.<br/>
I'm seeking a phd position in [Operation Management](https://en.wikipedia.org/wiki/Operations_management)&[Operation Research](https://en.wikipedia.org/wiki/Operations_research) in 2026/2027 fall. <br/> 
## Research Interest
I am thrilled by online learning, online optimization and experimental design, hoping to develop novel methods that can be used to solve real-world challenges. <br/>
## Publication & Working Paper
- Jierui Zuo, Hanzhang Qin (2025). [On Pareto Optimality for the Multinomial Logistic Bandit.](https://arxiv.org/abs/2501.19277) arxiv preprint 2501.19277 <br/><br/>
We provide a new online learning algorithm for tackling the Multinomial Logit Bandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial nature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based method that achieves Pareto optimality by balancing regret minimization and estimation error of the assortment revenues and the MNL parameters. We develop theoretical guarantees characterizing the tradeoff between regret and estimation error for the MNL-Bandit problem through information-theoretic bounds, and propose a modified UCB algorithm that incorporates forced exploration to improve parameter estimation accuracy while maintaining low regret. Our analysis sheds critical insights into how to optimally balance the collected revenues and the treatment estimation in dynamic assortment optimization.

## Contact
Email : zuojr22@mails.tsinghua.edu.cn<br/><br/>
Please push me to study math harder!<br/>
Last Update: 2025.2
